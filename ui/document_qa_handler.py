"""
æ–‡æ¡£é—®ç­”å¤„ç†æ¨¡å—
"""
import streamlit as st
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any

logger = logging.getLogger(__name__)

async def process_document_qa(uploaded_file, question, answer_style="detailed", include_quotes=True, confidence_threshold=0.7,
                            enable_advanced_confidence=False, use_rag=True, use_reranker=True, rag_top_k=12, rag_rerank_top_n=6):
    """å¤„ç†æ–‡æ¡£é—®ç­”"""
    try:
        from config.settings import get_config
        from mcp_services.base_service import handle_mcp_request
        from agents.base_agent import agent_coordinator
        from utils.rag_utils import (
            compute_file_id,
            build_or_load_index,
            retrieve_with_optional_rerank,
            build_context_from_chunks,
        )
        
        # è¿›åº¦æŒ‡ç¤º
        progress_bar = st.progress(0, text="å¼€å§‹å¤„ç†é—®ç­”...")
        
        # ä¿å­˜æ–‡ä»¶
        file_config = get_config("file")
        upload_dir = Path(file_config.get("upload_dir", "uploads"))
        upload_dir.mkdir(exist_ok=True)
        
        file_path = upload_dir / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        progress_bar.progress(25, text="ğŸ“ æ–‡æ¡£ä¿å­˜å®Œæˆ...")
        
        # è§£ææ–‡æ¡£
        progress_bar.progress(50, text="ğŸ“– æ­£åœ¨è§£ææ–‡æ¡£...")
        
        parse_result = await handle_mcp_request(
            method="document_parser/extract_text",
            params={"file_path": str(file_path)}
        )
        
        if parse_result.get("result", {}).get("success", False):
            text_content = parse_result["result"]["result"]["text_content"]
            
            # RAGå¤„ç†
            context_text = text_content
            if use_rag and text_content:
                try:
                    progress_bar.progress(60, text="ğŸ” æ„å»ºRAGç´¢å¼•...")
                    
                    # è®¡ç®—æ–‡ä»¶ID
                    file_id = compute_file_id(str(file_path))
                    
                    # æ„å»ºæˆ–åŠ è½½ç´¢å¼•
                    store, embedder, reranker = build_or_load_index(file_id, text_content)
                    
                    progress_bar.progress(70, text="ğŸ” RAGæ£€ç´¢ç›¸å…³å†…å®¹...")
                    
                    # æ£€ç´¢ç›¸å…³ç‰‡æ®µ
                    chunks = retrieve_with_optional_rerank(
                        query=question,
                        store=store,
                        embedder=embedder,
                        top_k=rag_top_k,
                        rerank_top_n=rag_rerank_top_n,
                        use_reranker=use_reranker
                    )
                    
                    if chunks:
                        # æ„å»ºä¸Šä¸‹æ–‡
                        context_text = build_context_from_chunks(chunks)
                        logger.info(f"RAGæ£€ç´¢åˆ°{len(chunks)}ä¸ªç›¸å…³ç‰‡æ®µ")
                    else:
                        logger.warning("RAGæœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
                        
                except Exception as e:
                    logger.error(f"RAGå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {e}")
                    # RAGå¤±è´¥æ—¶å›é€€åˆ°åŸå§‹æ–‡æœ¬
                    context_text = text_content
            
            progress_bar.progress(75, text="ğŸ¤– AIæ­£åœ¨æ€è€ƒç­”æ¡ˆ...")
            
            # æ‰§è¡Œé—®ç­”
            qa_input = {
                "document_content": context_text,
                "question": question,
                "document_type": Path(uploaded_file.name).suffix,
                "answer_style": answer_style,
                "include_quotes": include_quotes,
                "confidence_threshold": confidence_threshold,
                "use_rag": use_rag,
                "rag_chunks_count": len(chunks) if use_rag and 'chunks' in locals() else 0
            }
            
            qa_result = await agent_coordinator.execute_agent(
                "QA_Agent",
                qa_input
            )
            
            progress_bar.progress(100, text="âœ… é—®ç­”å®Œæˆï¼")
            
            if qa_result.get("success", False):
                from ui.result_display import display_qa_results
                display_qa_results(qa_result["result"])
            else:
                st.error(f"âŒ é—®ç­”å¤±è´¥: {qa_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                st.warning("ğŸ’¡ å»ºè®®é‡æ–°è¡¨è¿°é—®é¢˜æˆ–æ£€æŸ¥æ–‡æ¡£å†…å®¹")
        else:
            st.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")
            st.warning("ğŸ’¡ è¯·æ£€æŸ¥æ–‡æ¡£æ ¼å¼æ˜¯å¦æ­£ç¡®")
            
    except Exception as e:
        st.error(f"âŒ é—®ç­”å¤„ç†å¤±è´¥: {str(e)}")
        st.warning("ğŸ’¡ å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–æ›´æ¢æ–‡æ¡£")
        logger.error(f"é—®ç­”å¤„ç†å¤±è´¥: {e}")

async def process_mcp_qa(uploaded_file, question, mcp_agent, answer_style="detailed", 
                        include_quotes=True, confidence_threshold=0.7, max_iterations=10, show_thinking=True,
                        use_rag=True, use_reranker=True, rag_top_k=12, rag_rerank_top_n=6):
    """ä½¿ç”¨MCPæ™ºèƒ½ä½“å¤„ç†æ–‡æ¡£é—®ç­”"""
    try:
        from config.settings import get_config
        from mcp_services.base_service import handle_mcp_request
        from ui.status_manager import ConversationStatusManager, PerformanceMonitor
        from ui.streaming_components import StreamingChatInterface
        from utils.rag_utils import (
            compute_file_id,
            build_or_load_index,
            retrieve_with_optional_rerank,
            build_context_from_chunks,
        )
        
        # è¿›åº¦æŒ‡ç¤º
        status_manager = ConversationStatusManager()
        status_manager.start_conversation(max_iterations)
        
        performance_monitor = PerformanceMonitor()
        performance_monitor.start_monitoring()
        
        # ä¿å­˜æ–‡ä»¶
        file_config = get_config("file")
        upload_dir = Path(file_config.get("upload_dir", "uploads"))
        upload_dir.mkdir(exist_ok=True)
        
        file_path = upload_dir / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        status_manager.update_step("thinking", "æ–‡æ¡£å·²ä¿å­˜ï¼Œå¼€å§‹è§£æ...")
        
        # è§£ææ–‡æ¡£
        parse_result = await handle_mcp_request(
            method="document_parser/extract_text",
            params={"file_path": str(file_path)}
        )
        
        if parse_result.get("result", {}).get("success", False):
            text_content = parse_result["result"]["result"]["text_content"]
            
            # RAGå¤„ç†
            context_text = text_content
            rag_chunks_info = {"enabled": False, "chunks_count": 0}
            
            if use_rag and text_content:
                try:
                    status_manager.update_step("thinking", "æ„å»ºRAGç´¢å¼•...")
                    
                    # è®¡ç®—æ–‡ä»¶ID
                    file_id = compute_file_id(str(file_path))
                    
                    # æ„å»ºæˆ–åŠ è½½ç´¢å¼•
                    store, embedder, reranker = build_or_load_index(file_id, text_content)
                    
                    status_manager.update_step("thinking", "RAGæ£€ç´¢ç›¸å…³å†…å®¹...")
                    
                    # æ£€ç´¢ç›¸å…³ç‰‡æ®µ
                    chunks = retrieve_with_optional_rerank(
                        query=question,
                        store=store,
                        embedder=embedder,
                        top_k=rag_top_k,
                        rerank_top_n=rag_rerank_top_n,
                        use_reranker=use_reranker
                    )
                    
                    if chunks:
                        # æ„å»ºä¸Šä¸‹æ–‡
                        context_text = build_context_from_chunks(chunks)
                        rag_chunks_info = {"enabled": True, "chunks_count": len(chunks)}
                        logger.info(f"RAGæ£€ç´¢åˆ°{len(chunks)}ä¸ªç›¸å…³ç‰‡æ®µ")
                        status_manager.update_step("thinking", f"RAGæ£€ç´¢å®Œæˆï¼Œè·å¾—{len(chunks)}ä¸ªç›¸å…³ç‰‡æ®µ")
                    else:
                        logger.warning("RAGæœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
                        status_manager.update_step("thinking", "RAGæœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£")
                        
                except Exception as e:
                    logger.error(f"RAGå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {e}")
                    status_manager.update_step("thinking", f"RAGå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–‡æ¡£: {str(e)}")
                    # RAGå¤±è´¥æ—¶å›é€€åˆ°åŸå§‹æ–‡æœ¬
                    context_text = text_content
            
            status_manager.update_step("thinking", "æ–‡æ¡£è§£æå®Œæˆï¼Œå¯åŠ¨MCPæ™ºèƒ½ä½“...")
            
            # è®¾ç½®æ™ºèƒ½ä½“å‚æ•°
            mcp_agent.max_iterations = max_iterations
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            qa_input = {
                "question": question,
                "document_content": context_text,
                "document_type": Path(uploaded_file.name).suffix,
                "document_file_path": str(file_path),
                "answer_style": answer_style,
                "include_quotes": include_quotes,
                "confidence_threshold": confidence_threshold,
                "rag_info": rag_chunks_info
            }
            
            # åˆ›å»ºæµå¼èŠå¤©ç•Œé¢
            if show_thinking:
                chat_interface = StreamingChatInterface()
                
                # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹æµ - æ­£ç¡®ä¼ é€’å¼‚æ­¥ç”Ÿæˆå™¨
                logger.info("å¼€å§‹åˆ›å»ºMCPæ™ºèƒ½ä½“æ€è€ƒæµç¨‹")
                try:
                    # æ™ºèƒ½åˆå§‹åŒ–æ£€æŸ¥
                    if not mcp_agent.is_initialized():
                        logger.info("æ™ºèƒ½ä½“éœ€è¦åˆå§‹åŒ–...")
                        
                        # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
                        with st.spinner("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–MCPæ™ºèƒ½ä½“..."):
                            await mcp_agent.ensure_initialized()
                        
                        st.success("âœ… MCPæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
                    else:
                        logger.info(f"æ™ºèƒ½ä½“å·²åˆå§‹åŒ–ï¼Œ{len(mcp_agent.available_tools)}ä¸ªå·¥å…·å¯ç”¨")
                    
                    # åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºRAGä¿¡æ¯
                    if rag_chunks_info["enabled"]:
                        st.info(f"ğŸ” RAGå·²æ¿€æ´»ï¼Œæ£€ç´¢åˆ° {rag_chunks_info['chunks_count']} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    
                    # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨
                    thought_generator = mcp_agent.think_and_act(
                        question,
                        context_text,  # ä½¿ç”¨RAGå¤„ç†åçš„å†…å®¹
                        Path(uploaded_file.name).suffix,
                        str(file_path)
                    )
                    logger.info(f"æ€è€ƒç”Ÿæˆå™¨åˆ›å»ºæˆåŠŸ: {type(thought_generator)}")
                    
                    # æ˜¾ç¤ºæ€è€ƒæµç¨‹
                    final_answer = await chat_interface.display_thought_stream(thought_generator)
                    logger.info(f"æ€è€ƒæµç¨‹å®Œæˆï¼Œæœ€ç»ˆç­”æ¡ˆé•¿åº¦: {len(final_answer) if final_answer else 0}")
                    
                except Exception as e:
                    logger.error(f"MCPæ€è€ƒæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    st.error(f"âŒ MCPæ™ºèƒ½ä½“æ‰§è¡Œå¤±è´¥: {str(e)}")
                    return
                
                status_manager.complete_conversation(True)
                performance_monitor.end_monitoring()
                
                # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
                with st.expander("ğŸ“Š æ‰§è¡Œæ€§èƒ½æŠ¥å‘Š", expanded=False):
                    performance_monitor.show_performance_report()
                
                # æ˜¾ç¤ºçŠ¶æ€å†å²
                with st.expander("ğŸ“‹ è¯¦ç»†æ‰§è¡Œå†å²", expanded=False):
                    status_manager.show_status_history()
            
            else:
                # ä¸æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥å¤„ç†
                with st.spinner("ğŸ§  MCPæ™ºèƒ½ä½“æ­£åœ¨æ·±åº¦æ€è€ƒ..."):
                    # åœ¨ç®€åŒ–æ¨¡å¼ä¸‹ä¹Ÿæ˜¾ç¤ºRAGä¿¡æ¯
                    if rag_chunks_info["enabled"]:
                        st.info(f"ğŸ” RAGå·²æ¿€æ´»ï¼Œæ£€ç´¢åˆ° {rag_chunks_info['chunks_count']} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    
                    result = await mcp_agent.process(qa_input)
                    
                    if result.get("answer"):
                        st.markdown("### ğŸ¯ æœ€ç»ˆç­”æ¡ˆ")
                        st.write(result["answer"])
                        
                        # æ˜¾ç¤ºç®€åŒ–çš„ç»“æœä¿¡æ¯
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ€è€ƒè½®æ•°", result.get("iterations_used", 0))
                        with col2:
                            st.metric("å¯ç”¨å·¥å…·", result.get("tools_available", 0))
                        with col3:
                            st.metric("æ€è€ƒæ­¥éª¤", len(result.get("thought_processes", [])))
                
        else:
            st.error("âŒ æ–‡æ¡£è§£æå¤±è´¥")
            status_manager.complete_conversation(False)
            st.warning("ğŸ’¡ è¯·æ£€æŸ¥æ–‡æ¡£æ ¼å¼æ˜¯å¦æ­£ç¡®")
            
    except Exception as e:
        st.error(f"âŒ MCPé—®ç­”å¤„ç†å¤±è´¥: {str(e)}")
        st.warning("ğŸ’¡ å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–åˆ‡æ¢åˆ°ä¼ ç»Ÿé—®ç­”æ¨¡å¼")
        logger.error(f"MCPé—®ç­”å¤„ç†å¤±è´¥: {e}")
        
        if 'status_manager' in locals():
            status_manager.complete_conversation(False)
