"""
æ•°æ®åˆ†æå¤„ç†æ¨¡å—
"""
import streamlit as st
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, Any

logger = logging.getLogger(__name__)

async def process_data_analysis(uploaded_file, analysis_type, requirements, trend_params, **kwargs):
    """å¤„ç†æ•°æ®åˆ†æ"""
    # å¿½ç•¥é¢å¤–çš„æœªçŸ¥å‚æ•°
    if kwargs:
        logger.warning(f"æ”¶åˆ°æœªçŸ¥å‚æ•°ï¼Œå°†è¢«å¿½ç•¥: {list(kwargs.keys())}")
    
    progress_bar = st.progress(0, text="å¼€å§‹æ•°æ®åˆ†æ...")
    try:
        from agents.base_agent import agent_coordinator
        from ui.result_display import display_analysis_results
        
        try:
            df = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯: {e}")
            return
        
        progress_bar.progress(25, text="ğŸ“„ æ•°æ®åŠ è½½å®Œæˆ...")

        analysis_input = {
            "data": df,
            "analysis_type": analysis_type,
            "requirements": requirements,
            "source": uploaded_file.name,
            **trend_params # åˆå¹¶è¶‹åŠ¿åˆ†æå‚æ•°
        }
        progress_bar.progress(50, text="ğŸ¤– AIæ­£åœ¨è¿›è¡Œæ•°æ®åˆ†æ...")

        analysis_result = await agent_coordinator.execute_agent("Analysis_Agent", analysis_input)
        progress_bar.progress(100, text="âœ… åˆ†æå®Œæˆï¼")
        
        if analysis_result.get("success", False):
            display_analysis_results(analysis_result["result"])
            
            # ä¸ºä¼ ç»Ÿåˆ†æä¹Ÿç”Ÿæˆæ™ºèƒ½å›¾è¡¨
            try:
                from utils.chart_generator import ChartGenerator
                
                # ç”ŸæˆåŸºäºAIæ´å¯Ÿçš„å›¾è¡¨
                ai_insights = analysis_result["result"].get("ai_insights", "")
                if ai_insights and len(df) > 1:
                    chart_generator = ChartGenerator(df)
                    smart_charts = chart_generator.generate_charts_for_analysis(ai_insights)
                    
                    if smart_charts:
                        st.markdown("### ğŸ¤– AIæ™ºèƒ½å›¾è¡¨åˆ†æ")
                        st.info(f"ğŸ¨ åŸºäºAIæ´å¯Ÿï¼Œé¢å¤–ç”Ÿæˆäº† {len(smart_charts)} ä¸ªæ™ºèƒ½å›¾è¡¨")
                        
                        for chart_name, chart_fig in smart_charts.items():
                            if chart_fig:
                                with st.expander(f"ğŸ§  {chart_name.replace('_', ' ').title()}", expanded=False):
                                    st.plotly_chart(chart_fig, use_container_width=True)
                                    
            except Exception as chart_error:
                logger.warning(f"ä¼ ç»Ÿåˆ†ææ™ºèƒ½å›¾è¡¨ç”Ÿæˆå¤±è´¥: {chart_error}")
            
            # è®°å½•å¯¹è¯å†å²
            try:
                from utils.conversation_manager import conversation_manager
                result_text = str(analysis_result["result"])  # å°†åˆ†æç»“æœè½¬æ¢ä¸ºæ–‡æœ¬
                metadata = {
                    "file_name": uploaded_file.name,
                    "analysis_type": analysis_type,
                    "trend_params": trend_params,
                    "agent_type": "ä¼ ç»Ÿåˆ†æ",
                    "charts_generated": len(smart_charts) if 'smart_charts' in locals() else 0
                }
                # ä¼ é€’å›¾è¡¨æ•°æ®åˆ°å¯¹è¯ç®¡ç†å™¨
                charts_to_save = smart_charts if 'smart_charts' in locals() else {}
                conversation_manager.add_conversation(
                    requirements, result_text, "data_analysis", metadata, charts_to_save
                )
            except Exception as e:
                logger.warning(f"è®°å½•å¯¹è¯å†å²å¤±è´¥: {e}")
        else:
            st.error(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    except Exception as e:
        st.error(f"âŒ æ•°æ®åˆ†æå¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"æ•°æ®åˆ†æå¤„ç†å¤±è´¥: {e}", exc_info=True)

async def process_mcp_data_analysis(uploaded_file, analysis_requirements, mcp_agent, 
                                   max_iterations=10, show_thinking=True, confidence_threshold=0.7,
                                   use_rag=True, use_reranker=True, rag_top_k=12, rag_rerank_top_n=6, **kwargs):
    """ä½¿ç”¨MCPæ™ºèƒ½ä½“å¤„ç†æ•°æ®åˆ†æ"""
    # å¿½ç•¥é¢å¤–çš„æœªçŸ¥å‚æ•°
    if kwargs:
        logger.warning(f"æ”¶åˆ°æœªçŸ¥å‚æ•°ï¼Œå°†è¢«å¿½ç•¥: {list(kwargs.keys())}")
    
    try:
        from config.settings import get_config
        from ui.status_manager import ConversationStatusManager, PerformanceMonitor
        from ui.streaming_components import StreamingChatInterface
        from utils.rag_utils import (
            compute_file_id,
            build_or_load_index,
            retrieve_with_optional_rerank,
            build_context_from_chunks,
        )
        from utils.chart_generator import ChartGenerator, parse_chart_requests_from_text
        
        # è¿›åº¦æŒ‡ç¤º
        status_manager = ConversationStatusManager()
        status_manager.start_conversation(max_iterations)
        
        performance_monitor = PerformanceMonitor()
        performance_monitor.start_monitoring()
        
        # ä¿å­˜æ–‡ä»¶
        file_config = get_config("file")
        upload_dir = Path(file_config.get("upload_dir", "uploads"))
        upload_dir.mkdir(exist_ok=True)
        
        file_path = upload_dir / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        status_manager.update_step("thinking", "æ•°æ®æ–‡ä»¶å·²ä¿å­˜ï¼Œå¼€å§‹è§£æ...")
        
        # è¯»å–æ•°æ®
        try:
            df = pd.read_excel(file_path)
            data_json = df.to_json(orient='records', date_format='iso')
            
            # ä¸ºæ•°æ®åˆ†æåˆ›å»ºå¯è¯»çš„æ–‡æœ¬è¡¨ç¤º
            data_summary = f"""
æ•°æ®æ¦‚è§ˆ:
- æ€»è¡Œæ•°: {df.shape[0]}
- æ€»åˆ—æ•°: {df.shape[1]}
- åˆ—å: {', '.join(df.columns.tolist())}

æ•°æ®æ ·æœ¬ (å‰5è¡Œ):
{df.head().to_string()}

æ•°æ®ç»Ÿè®¡ä¿¡æ¯:
{df.describe().to_string()}
"""
            
            status_manager.update_step("thinking", f"æ•°æ®è§£æå®Œæˆï¼Œ{df.shape[0]}è¡Œ{df.shape[1]}åˆ—")
            
        except Exception as e:
            st.error(f"âŒ æ•°æ®è§£æå¤±è´¥: {str(e)}")
            status_manager.complete_conversation(False)
            return
        
        # RAGå¤„ç† - å¯¹äºæ•°æ®åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®æ‘˜è¦å’Œåˆ†æéœ€æ±‚è¿›è¡ŒRAGå¤„ç†
        context_text = data_json
        rag_chunks_info = {"enabled": False, "chunks_count": 0}
        
        if use_rag and len(data_summary) > 1000:  # åªæœ‰å½“æ•°æ®æ‘˜è¦è¶³å¤Ÿé•¿æ—¶æ‰ä½¿ç”¨RAG
            try:
                status_manager.update_step("thinking", "æ„å»ºæ•°æ®RAGç´¢å¼•...")
                
                # è®¡ç®—æ–‡ä»¶ID
                file_id = compute_file_id(str(file_path) + "_data_analysis")
                
                # ä½¿ç”¨æ•°æ®æ‘˜è¦æ„å»ºç´¢å¼•
                store, embedder, reranker = build_or_load_index(file_id, data_summary)
                
                status_manager.update_step("thinking", "RAGæ£€ç´¢ç›¸å…³æ•°æ®ä¿¡æ¯...")
                
                # æ£€ç´¢ç›¸å…³ç‰‡æ®µ
                chunks = retrieve_with_optional_rerank(
                    query=analysis_requirements,
                    store=store,
                    embedder=embedder,
                    top_k=rag_top_k,
                    rerank_top_n=rag_rerank_top_n,
                    use_reranker=use_reranker
                )
                
                if chunks:
                    # æ„å»ºä¸Šä¸‹æ–‡ï¼Œä½†ä»ä¿ç•™åŸå§‹JSONæ•°æ®
                    context_summary = build_context_from_chunks(chunks)
                    # åˆå¹¶RAGæ£€ç´¢çš„ä¸Šä¸‹æ–‡å’ŒåŸå§‹æ•°æ®
                    context_text = f"æ•°æ®ä¸Šä¸‹æ–‡:\n{context_summary}\n\nåŸå§‹æ•°æ®:\n{data_json}"
                    rag_chunks_info = {"enabled": True, "chunks_count": len(chunks)}
                    logger.info(f"æ•°æ®åˆ†æRAGæ£€ç´¢åˆ°{len(chunks)}ä¸ªç›¸å…³ç‰‡æ®µ")
                    status_manager.update_step("thinking", f"RAGæ£€ç´¢å®Œæˆï¼Œè·å¾—{len(chunks)}ä¸ªç›¸å…³æ•°æ®ç‰‡æ®µ")
                else:
                    logger.warning("æ•°æ®åˆ†æRAGæœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                    status_manager.update_step("thinking", "RAGæœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                    
            except Exception as e:
                logger.error(f"æ•°æ®åˆ†æRAGå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                status_manager.update_step("thinking", f"RAGå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®: {str(e)}")
                # RAGå¤±è´¥æ—¶å›é€€åˆ°åŸå§‹æ•°æ®
                context_text = data_json
        
        status_manager.update_step("thinking", "å¼€å§‹MCPæ™ºèƒ½ä½“åˆ†æ...")
        
        # è®¾ç½®æ™ºèƒ½ä½“å‚æ•°
        mcp_agent.max_iterations = max_iterations
        
        # åˆ›å»ºæµå¼èŠå¤©ç•Œé¢
        if show_thinking:
            chat_interface = StreamingChatInterface()
            
            # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹æµ
            logger.info("å¼€å§‹åˆ›å»ºMCPæ™ºèƒ½ä½“æ•°æ®åˆ†ææµç¨‹")
            try:
                # æ™ºèƒ½åˆå§‹åŒ–æ£€æŸ¥
                if not mcp_agent.is_initialized():
                    logger.info("æ™ºèƒ½ä½“éœ€è¦åˆå§‹åŒ–...")
                    
                    # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
                    with st.spinner("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–MCPæ™ºèƒ½ä½“..."):
                        await mcp_agent.ensure_initialized()
                    
                    st.success("âœ… MCPæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
                else:
                    logger.info(f"æ™ºèƒ½ä½“å·²åˆå§‹åŒ–ï¼Œ{len(mcp_agent.available_tools)}ä¸ªå·¥å…·å¯ç”¨")
                
                # åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºæ•°æ®å’ŒRAGä¿¡æ¯
                st.info(f"ğŸ“Š æ•°æ®å·²åŠ è½½: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
                if rag_chunks_info["enabled"]:
                    st.info(f"ğŸ” RAGå·²æ¿€æ´»ï¼Œæ£€ç´¢åˆ° {rag_chunks_info['chunks_count']} ä¸ªç›¸å…³æ•°æ®ç‰‡æ®µ")
                
                # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨ - ä½¿ç”¨RAGå¤„ç†åçš„æ•°æ®å†…å®¹
                thought_generator = mcp_agent.think_and_act(
                    analysis_requirements,
                    context_text,  # ä¼ é€’RAGå¤„ç†åçš„æ•°æ®å†…å®¹
                    Path(uploaded_file.name).suffix,
                    str(file_path)
                )
                logger.info(f"æ•°æ®åˆ†ææ€è€ƒç”Ÿæˆå™¨åˆ›å»ºæˆåŠŸ: {type(thought_generator)}")
                
                # æ˜¾ç¤ºæ€è€ƒæµç¨‹
                final_answer = await chat_interface.display_thought_stream(thought_generator)
                logger.info(f"æ•°æ®åˆ†ææ€è€ƒæµç¨‹å®Œæˆï¼Œæœ€ç»ˆç­”æ¡ˆé•¿åº¦: {len(final_answer) if final_answer else 0}")
                
                # ç”Ÿæˆæ™ºèƒ½å›¾è¡¨
                if final_answer:
                    try:
                        status_manager.update_step("thinking", "æ­£åœ¨ç”Ÿæˆæ™ºèƒ½å›¾è¡¨...")
                        
                        # åˆ›å»ºå›¾è¡¨ç”Ÿæˆå™¨
                        chart_generator = ChartGenerator(df)
                        
                        # åŸºäºåˆ†æç»“æœç”Ÿæˆå›¾è¡¨
                        generated_charts = chart_generator.generate_charts_for_analysis(final_answer)
                        
                        if generated_charts:
                            st.markdown("### ğŸ“Š æ™ºèƒ½ç”Ÿæˆçš„æ•°æ®å›¾è¡¨")
                            st.info(f"ğŸ¨ åŸºäºAIåˆ†æç»“æœï¼Œè‡ªåŠ¨ç”Ÿæˆäº† {len(generated_charts)} ä¸ªå›¾è¡¨")
                            
                            # æ˜¾ç¤ºæ¯ä¸ªå›¾è¡¨
                            for chart_name, chart_fig in generated_charts.items():
                                if chart_fig:
                                    # åˆ›å»ºå¯æŠ˜å çš„å›¾è¡¨åŒºåŸŸ
                                    with st.expander(f"ğŸ“ˆ {chart_name.replace('_', ' ').title()}", expanded=True):
                                        st.plotly_chart(chart_fig, use_container_width=True)
                            
                            logger.info(f"æˆåŠŸç”Ÿæˆå¹¶æ˜¾ç¤ºäº† {len(generated_charts)} ä¸ªå›¾è¡¨")
                        else:
                            st.info("ğŸ’¡ æœªç”Ÿæˆå›¾è¡¨ï¼Œå¯èƒ½æ˜¯æ•°æ®æ ¼å¼ä¸é€‚åˆå¯è§†åŒ–")
                    
                    except Exception as chart_error:
                        logger.warning(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œä½†åˆ†æç»§ç»­: {chart_error}")
                        st.warning("âš ï¸ å›¾è¡¨ç”Ÿæˆé‡åˆ°é—®é¢˜ï¼Œä½†åˆ†æç»“æœå·²å®Œæˆ")
                
            except Exception as e:
                logger.error(f"MCPæ•°æ®åˆ†ææ€è€ƒæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                st.error(f"âŒ MCPæ™ºèƒ½ä½“æ•°æ®åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
                return
            
            status_manager.complete_conversation(True)
            performance_monitor.end_monitoring()
            
            # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
            with st.expander("ğŸ“Š æ‰§è¡Œæ€§èƒ½æŠ¥å‘Š", expanded=False):
                performance_monitor.show_performance_report()
            
            # æ˜¾ç¤ºçŠ¶æ€å†å²
            with st.expander("ğŸ“‹ è¯¦ç»†æ‰§è¡Œå†å²", expanded=False):
                status_manager.show_status_history()
            
            # è®°å½•å¯¹è¯å†å²
            try:
                from utils.conversation_manager import conversation_manager
                metadata = {
                    "file_name": uploaded_file.name,
                    "max_iterations": max_iterations,
                    "confidence_threshold": confidence_threshold,
                    "use_rag": use_rag,
                    "use_reranker": use_reranker,
                    "agent_type": "MCPæ™ºèƒ½åŠ©æ‰‹",
                    "data_shape": f"{df.shape[0]}è¡ŒÃ—{df.shape[1]}åˆ—",
                    "charts_generated": len(generated_charts) if 'generated_charts' in locals() else 0
                }
                # ä¼ é€’å›¾è¡¨æ•°æ®åˆ°å¯¹è¯ç®¡ç†å™¨
                charts_to_save = generated_charts if 'generated_charts' in locals() else {}
                conversation_manager.add_conversation(
                    analysis_requirements, final_answer or "åˆ†æå®Œæˆ", "data_analysis", metadata, charts_to_save
                )
            except Exception as e:
                logger.warning(f"è®°å½•å¯¹è¯å†å²å¤±è´¥: {e}")
        
        else:
            # ä¸æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥å¤„ç†
            with st.spinner("ğŸ§  MCPæ™ºèƒ½ä½“æ­£åœ¨æ·±åº¦åˆ†ææ•°æ®..."):
                # åœ¨ç®€åŒ–æ¨¡å¼ä¸‹ä¹Ÿæ˜¾ç¤ºæ•°æ®å’ŒRAGä¿¡æ¯
                st.info(f"ğŸ“Š æ•°æ®å·²åŠ è½½: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")
                if rag_chunks_info["enabled"]:
                    st.info(f"ğŸ” RAGå·²æ¿€æ´»ï¼Œæ£€ç´¢åˆ° {rag_chunks_info['chunks_count']} ä¸ªç›¸å…³æ•°æ®ç‰‡æ®µ")
                
                # è¿™é‡Œå¯ä»¥è°ƒç”¨mcp_agentçš„åŒæ­¥æ–¹æ³•ï¼Œä½†ç›®å‰ä½¿ç”¨think_and_actå¹¶å¿½ç•¥æµ
                result = await mcp_agent.process({
                    "question": analysis_requirements,
                    "document_content": context_text,
                    "document_type": Path(uploaded_file.name).suffix,
                    "document_file_path": str(file_path),
                    "confidence_threshold": confidence_threshold,
                    "rag_info": rag_chunks_info
                })
                
                if result.get("answer"):
                    st.markdown("### ğŸ¯ æ•°æ®åˆ†æç»“æœ")
                    st.write(result["answer"])
                    
                    # ç”Ÿæˆæ™ºèƒ½å›¾è¡¨ï¼ˆç®€åŒ–æ¨¡å¼ï¼‰
                    try:
                        from utils.chart_generator import ChartGenerator
                        chart_generator = ChartGenerator(df)
                        generated_charts = chart_generator.generate_charts_for_analysis(result["answer"])
                        
                        if generated_charts:
                            st.markdown("### ğŸ“Š è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®å›¾è¡¨")
                            for chart_name, chart_fig in generated_charts.items():
                                if chart_fig:
                                    with st.expander(f"ğŸ“ˆ {chart_name.replace('_', ' ').title()}", expanded=False):
                                        st.plotly_chart(chart_fig, use_container_width=True)
                    except Exception as chart_error:
                        logger.warning(f"ç®€åŒ–æ¨¡å¼å›¾è¡¨ç”Ÿæˆå¤±è´¥: {chart_error}")
                    
                    # æ˜¾ç¤ºç®€åŒ–çš„ç»“æœä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ€è€ƒè½®æ•°", result.get("iterations_used", 0))
                    with col2:
                        st.metric("å¯ç”¨å·¥å…·", result.get("tools_available", 0))
                    with col3:
                        st.metric("æ€è€ƒæ­¥éª¤", len(result.get("thought_processes", [])))
                    
                    # è®°å½•å¯¹è¯å†å²
                    try:
                        from utils.conversation_manager import conversation_manager
                        metadata = {
                            "file_name": uploaded_file.name,
                            "max_iterations": max_iterations,
                            "confidence_threshold": confidence_threshold,
                            "use_rag": use_rag,
                            "use_reranker": use_reranker,
                            "agent_type": "MCPæ™ºèƒ½åŠ©æ‰‹(ç®€åŒ–æ¨¡å¼)",
                            "data_shape": f"{df.shape[0]}è¡ŒÃ—{df.shape[1]}åˆ—",
                            "iterations_used": result.get("iterations_used", 0),
                            "charts_generated": len(generated_charts) if 'generated_charts' in locals() else 0
                        }
                        # ä¼ é€’å›¾è¡¨æ•°æ®åˆ°å¯¹è¯ç®¡ç†å™¨
                        charts_to_save = generated_charts if 'generated_charts' in locals() else {}
                        conversation_manager.add_conversation(
                            analysis_requirements, result["answer"], "data_analysis", metadata, charts_to_save
                        )
                    except Exception as e:
                        logger.warning(f"è®°å½•å¯¹è¯å†å²å¤±è´¥: {e}")
    
    except Exception as e:
        st.error(f"âŒ MCPæ•°æ®åˆ†æå¤„ç†å¤±è´¥: {str(e)}")
        st.warning("ğŸ’¡ å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·å°è¯•ç®€åŒ–åˆ†æè¦æ±‚æˆ–æ£€æŸ¥æ•°æ®æ ¼å¼")
        logger.error(f"MCPæ•°æ®åˆ†æå¤„ç†å¤±è´¥: {e}")
        
        if 'status_manager' in locals():
            status_manager.complete_conversation(False)
